{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "def create_dataset(path):\n",
    "    dataset = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            name = file.split(\"__\")[1]\n",
    "            if \"_\" not in name and \"f\" not in name and \"r\" not in name:\n",
    "                file = file.split(\".\")[0]\n",
    "                for i in range(10):\n",
    "                    input1 = root + \"/\" + file + \".png\"\n",
    "                    input2 = root + \"/\" + file + \"_\" + str(i) + \".png\"\n",
    "                    GT = root + \"/\" + file + \"_\" + str(i) + \"_GT.png\"\n",
    "                    dataset.append((input1, input2, GT))\n",
    "            elif (\"_flip_\" in name or \"_rot_\" in name) and \"GT\" not in name:\n",
    "                file = file.split(\".\")[0]\n",
    "                if file[-1] in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n",
    "                    continue\n",
    "                for i in range(10):\n",
    "                    input1 = root + \"/\" + file + \".png\"\n",
    "                    input2 = root + \"/\" + file + \"_\" + str(i) + \".png\"\n",
    "                    GT = root + \"/\" + file + \"_\" + str(i) + \"_GT.png\"\n",
    "                    dataset.append((input1, input2, GT))\n",
    "    return dataset\n",
    "\n",
    "train = create_dataset(\"train\")\n",
    "test = create_dataset(\"test\")\n",
    "validation = create_dataset(\"validation\")\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "x_validation = []\n",
    "y_validation = []\n",
    "\n",
    "for i in range(0, len(train), 8): #TODO:\n",
    "    x_train.append((train[i][0], train[i][1]))\n",
    "    y_train.append(train[i][2])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    x_test.append((test[i][0], test[i][1]))\n",
    "    y_test.append(test[i][2])\n",
    "\n",
    "for i in range(0, len(validation), 8):\n",
    "    x_validation.append((validation[i][0], validation[i][1]))\n",
    "    y_validation.append(validation[i][2])\n",
    "\n",
    "def get_batch(x, y, batch_size):\n",
    "    # shuffle x and y\n",
    "    zipped = list(zip(x, y))\n",
    "    random.shuffle(zipped)\n",
    "    x, y = zip(*zipped)\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        batch.append((x[i], y[i]))\n",
    "    return batch\n",
    "\n",
    "def get_non_random_batch(x, y, batch_size, start_index):\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        batch.append((x[start_index + i], y[start_index + i]))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search functions and planners evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import heapq\n",
    "\n",
    "# Define the heuristic functions\n",
    "def manhattan_distance(start, goal):\n",
    "    return abs(start[0] - goal[0]) + abs(start[1] - goal[1])\n",
    "\n",
    "def focal_search(w, grid, start, goal, probability_map, cf, cf_map, eps):\n",
    "    # Initialize the open and closed lists\n",
    "    open_list = []\n",
    "    focal_list = []\n",
    "    closed_list = set()\n",
    "\n",
    "    # Add the start node to the open list with the priority equal to the heuristic\n",
    "    heapq.heappush(open_list, (manhattan_distance(start, goal), 0, start))\n",
    "    focal_list.append((manhattan_distance(start, goal), probability_map[start[0]][start[1]], start))\n",
    "    \n",
    "    came_from = {start: None}\n",
    "    \n",
    "    g_costs = {start: 0}\n",
    "    \n",
    "    while open_list:\n",
    "        if focal_list:\n",
    "            focal_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            current_f, current_g, current = focal_list.pop(0)\n",
    "            # remove from open list\n",
    "            open_list = [node for node in open_list if node[2] != current]\n",
    "            heapq.heapify(open_list)\n",
    "        else:\n",
    "            current_f, current_g, current = heapq.heappop(open_list)\n",
    "        \n",
    "        closed_list.add(current)\n",
    "        \n",
    "        if current == goal:\n",
    "            path = []\n",
    "            while current is not None:\n",
    "                path.append(current)\n",
    "                current = came_from[current]\n",
    "            return path[::-1], closed_list\n",
    "        \n",
    "        # Update focal list if necessary\n",
    "        new_focal_list = []\n",
    "        f_min = open_list[0][0] if open_list else float('inf')\n",
    "        for node in open_list:\n",
    "            if node[0] <= eps * f_min:\n",
    "                new_focal_list.append((node[0], probability_map[node[2][0]][node[2][1]], node[2]))\n",
    "        focal_list = new_focal_list\n",
    "        \n",
    "        # Check the neighbors of the current node\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            neighbor = (current[0] + dx, current[1] + dy)\n",
    "            \n",
    "            # Ensure the neighbor is within the grid bounds and is walkable\n",
    "            if 0 <= neighbor[0] < len(grid) and 0 <= neighbor[1] < len(grid[0]) and grid[neighbor[0]][neighbor[1]] == 1:\n",
    "                if neighbor in closed_list:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate the tentative g cost\n",
    "                tentative_g_cost = g_costs[current] + 1\n",
    "                \n",
    "                if neighbor not in g_costs or tentative_g_cost < g_costs[neighbor]:\n",
    "                    # Update the cost and path\n",
    "                    g_costs[neighbor] = tentative_g_cost\n",
    "                    if cf:\n",
    "                        if cf_map[neighbor[0]][neighbor[1]] > 0:\n",
    "                            f_cost = tentative_g_cost + w * manhattan_distance(neighbor, goal) / cf_map[neighbor[0]][neighbor[1]]\n",
    "                        else:\n",
    "                            cf_map[neighbor[0]][neighbor[1]] = 0.001\n",
    "                            f_cost = tentative_g_cost + w * manhattan_distance(neighbor, goal) / cf_map[neighbor[0]][neighbor[1]]\n",
    "                    else:\n",
    "                        f_cost = tentative_g_cost + w * manhattan_distance(neighbor, goal)\n",
    "                    heapq.heappush(open_list, (f_cost, tentative_g_cost, neighbor))\n",
    "                    if f_cost <= eps * f_min:\n",
    "                        focal_list.append((f_cost, probability_map[neighbor[0]][neighbor[1]], neighbor))\n",
    "                    came_from[neighbor] = current\n",
    "    # If the goal was not reached, return None\n",
    "    return None, closed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(node1, node2):\n",
    "    return abs(node1[0] - node2[0]) + abs(node1[1] - node2[1])\n",
    "\n",
    "def get_neighbors(node, input_image):\n",
    "    neighbors = []\n",
    "    for dy, dx in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        new_y, new_x = node[1] + dy, node[0] + dx\n",
    "        if new_y >= 0 and new_y < input_image.shape[1] and new_x >= 0 and new_x < input_image.shape[0] and input_image[new_x, new_y] == 1:\n",
    "            neighbors.append((new_x, new_y))\n",
    "    return neighbors\n",
    "\n",
    "def a_star_GT(w, input_image, start, target):\n",
    "    # run a_star algorithm from target to start\n",
    "    shape = input_image.shape\n",
    "    g_values = {target: 0}\n",
    "    for y in range(shape[0]):\n",
    "        for x in range(shape[1]):\n",
    "            if (y, x) != target:\n",
    "                g_values[(y, x)] = float('inf')\n",
    "    open_list = [(target, 0, None)]\n",
    "    closed = []\n",
    "    \n",
    "    while open_list:\n",
    "        node = open_list[0]\n",
    "        open_list.remove(node)\n",
    "        closed.append(node)\n",
    "        if node[0] == start:\n",
    "            path = []\n",
    "            while node is not None:\n",
    "                path.append(node[0])\n",
    "                node = node[2]\n",
    "            path.reverse()\n",
    "            return path, closed\n",
    "        \n",
    "        for neigh in get_neighbors(node[0], input_image):\n",
    "            if g_values[node[0]] + 1 < g_values[neigh]:\n",
    "                g_values[neigh] = g_values[node[0]] + 1\n",
    "                f = g_values[neigh] + w * manhattan_distance(neigh, start)\n",
    "                if (neigh, f, node) not in closed:\n",
    "                    open_list.append((neigh, f, node))\n",
    "    return None, closed\n",
    "\n",
    "# ici\n",
    "def a_star_manhattan(w, input_image, start, target):\n",
    "    import heapq\n",
    "    \n",
    "    open = []\n",
    "    closed = set()\n",
    "    g_values = {}\n",
    "\n",
    "    heapq.heappush(open, (0, start, None))\n",
    "    g_values[start] = 0\n",
    "    \n",
    "    img_size = input_image.shape\n",
    "    \n",
    "    while open:\n",
    "        current = heapq.heappop(open)\n",
    "        current_f, current_node, current_parent = current\n",
    "        \n",
    "        closed.add(current_node)\n",
    "        \n",
    "        if current_node == target:\n",
    "            path = []\n",
    "            while current:\n",
    "                path.append(current[1])\n",
    "                current = current[2]\n",
    "            return path[::-1], closed\n",
    "        \n",
    "        for dy, dx in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "            new_y, new_x = current_node[1] + dy, current_node[0] + dx\n",
    "            neighbor = (new_x, new_y)\n",
    "            if (new_y < 0 or new_y >= img_size[1] or new_x < 0 or new_x >= img_size[1] or input_image[neighbor] == 0 or neighbor in closed):\n",
    "                continue\n",
    "            \n",
    "            tentative_g = g_values[current_node] + 1\n",
    "            if neighbor not in g_values or tentative_g < g_values[neighbor]:\n",
    "                g_values[neighbor] = tentative_g\n",
    "                f = tentative_g + w * manhattan_distance(neighbor, target)\n",
    "                heapq.heappush(open, (f, neighbor, current))\n",
    "    return None, closed\n",
    "\n",
    "def a_star(w, input_image, start, target, output):\n",
    "    import heapq\n",
    "    def is_valid(x, y):\n",
    "        return x >= 0 and x < img_size[0] and y >= 0 and y < img_size[1] and input_image[x, y] != 0\n",
    "    \n",
    "    def manhattan_distance(a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "    \n",
    "    open = []\n",
    "    closed = set()\n",
    "    g_values = {start: 0}\n",
    "    parent_map = {}\n",
    "\n",
    "    heapq.heappush(open, (0, start))\n",
    "    open_set = {start}\n",
    "    \n",
    "    img_size = input_image.shape\n",
    "    \n",
    "    while open:\n",
    "        current_f, current_node = heapq.heappop(open)\n",
    "        open_set.remove(current_node)\n",
    "        \n",
    "        closed.add(current_node)\n",
    "        \n",
    "        if current_node == target:\n",
    "            path = []\n",
    "            while current_node:\n",
    "                path.append(current_node)\n",
    "                current_node = parent_map.get(current_node)\n",
    "            return path[::-1], closed\n",
    "        \n",
    "        for dy, dx in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "            neighbor = (current_node[0] + dx, current_node[1] + dy)\n",
    "            if not is_valid(*neighbor) or neighbor in closed:\n",
    "                continue\n",
    "            \n",
    "            tentative_g = g_values[current_node] + 1\n",
    "            if neighbor not in g_values or tentative_g < g_values[neighbor]:\n",
    "                g_values[neighbor] = tentative_g\n",
    "                cf = output[neighbor]\n",
    "                if cf == 0:\n",
    "                    cf = 0.01\n",
    "                f = tentative_g + w * manhattan_distance(neighbor, target) / cf\n",
    "                parent_map[neighbor] = current_node\n",
    "                if neighbor not in open_set:\n",
    "                    heapq.heappush(open, (f, neighbor))\n",
    "                    open_set.add(neighbor)\n",
    "    return None, closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def everything_new(w, input_image, start, target, output_ppm, output_cf, eps):\n",
    "    import time\n",
    "    rt_cf = time.time()\n",
    "    path_pred_cf, visited_cf = a_star(w, input_image, start, target, output_cf)\n",
    "    rt_cf = time.time() - rt_cf\n",
    "    rt_ppm_man = time.time()\n",
    "    path_pred_ppm_man, visited_ppm_man = focal_search(w, input_image, start, target, output_ppm, False, None, eps)\n",
    "    rt_ppm_man = time.time() - rt_ppm_man\n",
    "    rt_ppm_cf = time.time()\n",
    "    path_pred_ppm_cf, visited_ppm_cf = focal_search(w, input_image, start, target, output_ppm, True, output_cf, eps)\n",
    "    rt_ppm_cf = time.time() - rt_ppm_cf\n",
    "    path_pred_GT, _ = a_star_GT(1, input_image, start, target)\n",
    "    rt_man = time.time()\n",
    "    path_pred_man, visited_man = a_star_manhattan(1, input_image, start, target)\n",
    "    rt_man = time.time() - rt_man\n",
    "    \n",
    "    # test output_cf - output_ppm\n",
    "    rt_cf_ppm_25 = time.time()\n",
    "    path_pred_cf_ppm25, visited_cf_ppm25 = a_star(w, input_image, start, target, output_cf + 0.25 * output_ppm)\n",
    "    rt_cf_ppm_25 = time.time() - rt_cf_ppm_25\n",
    "    rt_cf_ppm_50 = time.time()\n",
    "    path_pred_cf_ppm50, visited_cf_ppm50 = a_star(w, input_image, start, target, output_cf + 0.50 * output_ppm)\n",
    "    rt_cf_ppm_50 = time.time() - rt_cf_ppm_50\n",
    "    rt_cf_ppm_75 = time.time()\n",
    "    path_pred_cf_ppm75, visited_cf_ppm75 = a_star(w, input_image, start, target, output_cf + 0.75 * output_ppm)\n",
    "    rt_cf_ppm_75 = time.time() - rt_cf_ppm_75\n",
    "    rt_cf_ppm_1 = time.time()\n",
    "    path_pred_cf_ppm1, visited_cf_ppm1 = a_star(w, input_image, start, target, output_cf + 1 * output_ppm)\n",
    "    rt_cf_ppm_1 = time.time() - rt_cf_ppm_1\n",
    "    # if output_ppm < 0.5, make its value to 1\n",
    "    output_ppm1 = output_ppm.copy()\n",
    "    output_ppm1[output_ppm1 < 0.75] = 10\n",
    "    out_cf_sur_ppm = output_cf / output_ppm1\n",
    "    rt_cf_sur_ppm = time.time()\n",
    "    path_pred_cf_sur_ppm, visited_cf_sur_ppm = a_star(w, input_image, start, target, out_cf_sur_ppm)\n",
    "    rt_cf_sur_ppm = time.time() - rt_cf_sur_ppm\n",
    "    \n",
    "    \"\"\" plt.imshow(output_cf, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(output_ppm, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(out_cf_sur_ppm, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(out_cf_fois_ppm, cmap='gray')\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "    if path_pred_cf is not None and path_pred_ppm_man is not None and path_pred_ppm_cf is not None and path_pred_GT is not None and path_pred_man is not None and path_pred_cf_ppm25 is not None and path_pred_cf_ppm50 is not None and path_pred_cf_ppm75 is not None and path_pred_cf_ppm1 is not None and path_pred_cf_sur_ppm is not None:\n",
    "        count_visited_cf = len(visited_cf)\n",
    "        count_visited_ppm_man = len(visited_ppm_man)\n",
    "        count_visited_ppm_cf = len(visited_ppm_cf)\n",
    "        count_visited_GT = len(path_pred_GT)\n",
    "        count_visited_man = len(visited_man)\n",
    "        count_visited_cf_ppm25 = len(visited_cf_ppm25)\n",
    "        count_visited_cf_ppm50 = len(visited_cf_ppm50)\n",
    "        count_visited_cf_ppm75 = len(visited_cf_ppm75)\n",
    "        count_visited_cf_ppm1 = len(visited_cf_ppm1)\n",
    "        count_visited_cf_sur_ppm = len(visited_cf_sur_ppm)\n",
    "        \n",
    "        return len(path_pred_cf), len(path_pred_ppm_man), len(path_pred_ppm_cf), len(path_pred_GT), len(path_pred_man), len(path_pred_cf_ppm25), len(path_pred_cf_ppm50), len(path_pred_cf_ppm75), len(path_pred_cf_ppm1), len(path_pred_cf_sur_ppm), count_visited_cf, count_visited_ppm_man, count_visited_ppm_cf, count_visited_GT, count_visited_man, count_visited_cf_ppm25, count_visited_cf_ppm50, count_visited_cf_ppm75, count_visited_cf_ppm1, count_visited_cf_sur_ppm, rt_cf, rt_ppm_man, rt_ppm_cf, rt_man, rt_cf_ppm_25, rt_cf_ppm_50, rt_cf_ppm_75, rt_cf_ppm_1, rt_cf_sur_ppm\n",
    "    return None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP and Tiled-MP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 64x64 images\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2DBlock_ppm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv2DBlock_ppm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResNetBlock_ppm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResNetBlock_ppm, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class DownsampleBlock_ppm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownsampleBlock_ppm, self).__init__()\n",
    "        self.downsample = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class UpsampleBlock_ppm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpsampleBlock_ppm, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class TransformerBlock_ppm(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock_ppm, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_size * 4, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.permute(2, 0, 1)  # (B, H*W, C) -> (H*W, B, C)\n",
    "        x_cal = self.norm1(x)\n",
    "        # multi head selfattention with 4 heads\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        #do it again\n",
    "        x_cal = self.norm1(x)\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        # layer normalization\n",
    "        x_cal = self.norm2(x)\n",
    "        # feed forward network\n",
    "        x_cal = self.ff(x_cal)\n",
    "        return x + x_cal\n",
    "\n",
    "class Encoder_ppm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder_ppm, self).__init__()\n",
    "        self.initial_conv = Conv2DBlock_ppm(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock_ppm(64),\n",
    "                DownsampleBlock_ppm(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, c, x.device)\n",
    "        \n",
    "        x = x.flatten(2).permute(2, 0, 1)  # (B, C, H, W) -> (H*W, B, C)\n",
    "        x = x + positional_embeddings\n",
    "        return x, h, w\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(height * width, 1, channels, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class Decoder_ppm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder_ppm, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock_ppm(64),\n",
    "                UpsampleBlock_ppm(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        x = x.permute(1, 2, 0).view(-1, 64, h, w)  # (H*W, B, C) -> (B, C, H, W)\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, 64, x.device)\n",
    "        \n",
    "        x = x + positional_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.tanh(self.final_conv(x))\n",
    "        return x\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(1, channels, height, width, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class PPMModel_ppm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPMModel_ppm, self).__init__()\n",
    "        self.encoder = Encoder_ppm()\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock_ppm(64, 8) for _ in range(4)])\n",
    "        self.decoder = Decoder_ppm()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, h, w = self.encoder(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = self.decoder(x, h, w)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Conv2DBlock_cf(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv2DBlock_cf, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResNetBlock_cf(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResNetBlock_cf, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class DownsampleBlock_cf(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownsampleBlock_cf, self).__init__()\n",
    "        self.downsample = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class UpsampleBlock_cf(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpsampleBlock_cf, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class TransformerBlock_cf(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock_cf, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_size * 4, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.permute(2, 0, 1)  # (B, H*W, C) -> (H*W, B, C)\n",
    "        x_cal = self.norm1(x)\n",
    "        # multi head selfattention with 4 heads\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        #do it again\n",
    "        x_cal = self.norm1(x)\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        # layer normalization\n",
    "        x_cal = self.norm2(x)\n",
    "        # feed forward network\n",
    "        x_cal = self.ff(x_cal)\n",
    "        return x + x_cal\n",
    "\n",
    "class Encoder_cf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder_cf, self).__init__()\n",
    "        self.initial_conv = Conv2DBlock_cf(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock_cf(64),\n",
    "                DownsampleBlock_cf(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, c, x.device)\n",
    "        \n",
    "        x = x.flatten(2).permute(2, 0, 1)  # (B, C, H, W) -> (H*W, B, C)\n",
    "        x = x + positional_embeddings\n",
    "        return x, h, w\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(height * width, 1, channels, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class Decoder_cf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder_cf, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock_cf(64),\n",
    "                UpsampleBlock_cf(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        x = x.permute(1, 2, 0).view(-1, 64, h, w)  # (H*W, B, C) -> (B, C, H, W)\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, 64, x.device)\n",
    "        \n",
    "        x = x + positional_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.tanh(self.final_conv(x))\n",
    "        return x\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(1, channels, height, width, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class PPMModel_cf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPMModel_cf, self).__init__()\n",
    "        self.encoder = Encoder_cf()\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock_cf(64, 8) for _ in range(4)])\n",
    "        self.decoder = Decoder_cf()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, h, w = self.encoder(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = self.decoder(x, h, w)\n",
    "        return x\n",
    "\n",
    "# Evaluate on 64x64 images\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownsampleBlock, self).__init__()\n",
    "        self.downsample = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_size * 4, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.permute(2, 0, 1)  # (B, H*W, C) -> (H*W, B, C)\n",
    "        x_cal = self.norm1(x)\n",
    "        # multi head selfattention with 4 heads\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        #do it again\n",
    "        x_cal = self.norm1(x)\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        # layer normalization\n",
    "        x_cal = self.norm2(x)\n",
    "        # feed forward network\n",
    "        x_cal = self.ff(x_cal)\n",
    "        return x + x_cal\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.initial_conv = Conv2DBlock(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock(64),\n",
    "                DownsampleBlock(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, c, x.device)\n",
    "        \n",
    "        x = x.flatten(2).permute(2, 0, 1)  # (B, C, H, W) -> (H*W, B, C)\n",
    "        x = x + positional_embeddings\n",
    "        return x, h, w\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(height * width, 1, channels, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock(64),\n",
    "                UpsampleBlock(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        x = x.permute(1, 2, 0).view(-1, 64, h, w)  # (H*W, B, C) -> (B, C, H, W)\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, 64, x.device)\n",
    "        \n",
    "        x = x + positional_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.tanh(self.final_conv(x))\n",
    "        return x\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(1, channels, height, width, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class PPMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPMModel, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(64, 8) for _ in range(4)])\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, h, w = self.encoder(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = self.decoder(x, h, w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DBlock_cf2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv2DBlock_cf2, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResNetBlock_cf2(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResNetBlock_cf2, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(32, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class DownsampleBlock_cf2(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownsampleBlock_cf2, self).__init__()\n",
    "        self.downsample = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class UpsampleBlock_cf2(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpsampleBlock_cf2, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class TransformerBlock_cf2(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock_cf2, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_size * 4, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.permute(2, 0, 1)  # (B, H*W, C) -> (H*W, B, C)\n",
    "        x_cal = self.norm1(x)\n",
    "        # multi head selfattention with 4 heads\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        #do it again\n",
    "        x_cal = self.norm1(x)\n",
    "        x_cal = self.attention(x_cal, x_cal, x_cal)[0]\n",
    "        x = x + x_cal\n",
    "        # layer normalization\n",
    "        x_cal = self.norm2(x)\n",
    "        # feed forward network\n",
    "        x_cal = self.ff(x_cal)\n",
    "        return x + x_cal\n",
    "\n",
    "class Encoder_cf2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder_cf2, self).__init__()\n",
    "        self.initial_conv = Conv2DBlock_cf(2, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock_cf2(64),\n",
    "                DownsampleBlock_cf2(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, c, x.device)\n",
    "        \n",
    "        x = x.flatten(2).permute(2, 0, 1)  # (B, C, H, W) -> (H*W, B, C)\n",
    "        x = x + positional_embeddings\n",
    "        return x, h, w\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(height * width, 1, channels, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class Decoder_cf2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder_cf2, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResNetBlock_cf2(64),\n",
    "                UpsampleBlock_cf2(64)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        x = x.permute(1, 2, 0).view(-1, 64, h, w)  # (H*W, B, C) -> (B, C, H, W)\n",
    "        \n",
    "        # Create positional embeddings dynamically based on spatial dimensions\n",
    "        positional_embeddings = self.get_positional_embeddings(h, w, 64, x.device)\n",
    "        \n",
    "        x = x + positional_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.tanh(self.final_conv(x))\n",
    "        return x\n",
    "    \n",
    "    def get_positional_embeddings(self, height, width, channels, device):\n",
    "        pos_embed = torch.randn(1, channels, height, width, device=device)\n",
    "        return pos_embed\n",
    "\n",
    "class PPMModel_cf2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPMModel_cf2, self).__init__()\n",
    "        self.encoder = Encoder_cf2()\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock_cf2(64, 8) for _ in range(4)])\n",
    "        self.decoder = Decoder_cf2()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, h, w = self.encoder(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = self.decoder(x, h, w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000/32000 [7:11:36<00:00,  1.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found: 9696\n",
      "       Instance Complexity  Optimal Found CF  Optimal Found PPM+MAN  \\\n",
      "count         86304.000000      86304.000000           86304.000000   \n",
      "mean              1.157167         88.453606              73.150723   \n",
      "std               0.254528         31.958286              44.317793   \n",
      "min               1.016129          0.000000               0.000000   \n",
      "25%               1.027027        100.000000               0.000000   \n",
      "50%               1.031250        100.000000             100.000000   \n",
      "75%               1.166667        100.000000             100.000000   \n",
      "max               2.967742        100.000000             100.000000   \n",
      "\n",
      "       Optimal Found PPM+CF  Optimal Found CF+0.25*PPM  \\\n",
      "count          86304.000000               86304.000000   \n",
      "mean              72.234195                  93.090703   \n",
      "std               44.784638                  25.361364   \n",
      "min                0.000000                   0.000000   \n",
      "25%                0.000000                 100.000000   \n",
      "50%              100.000000                 100.000000   \n",
      "75%              100.000000                 100.000000   \n",
      "max              100.000000                 100.000000   \n",
      "\n",
      "       Optimal Found CF+0.50*PPM  Optimal Found CF+0.75*PPM  \\\n",
      "count               86304.000000               86304.000000   \n",
      "mean                   92.986420                  92.869392   \n",
      "std                    25.537722                  25.733692   \n",
      "min                     0.000000                   0.000000   \n",
      "25%                   100.000000                 100.000000   \n",
      "50%                   100.000000                 100.000000   \n",
      "75%                   100.000000                 100.000000   \n",
      "max                   100.000000                 100.000000   \n",
      "\n",
      "       Optimal Found CF+1*PPM  Optimal Found CF/PPM  Predicted Path Length CF  \\\n",
      "count            86304.000000          86304.000000              86304.000000   \n",
      "mean                92.840425             86.092186                 41.687384   \n",
      "std                 25.781888             34.603006                 10.886831   \n",
      "min                  0.000000              0.000000                 31.000000   \n",
      "25%                100.000000            100.000000                 34.000000   \n",
      "50%                100.000000            100.000000                 39.000000   \n",
      "75%                100.000000            100.000000                 46.000000   \n",
      "max                100.000000            100.000000                139.000000   \n",
      "\n",
      "       ...  Run Time PPM+MAN  Run Time PPM+CF  Run Time Manhattan  \\\n",
      "count  ...      86304.000000     86304.000000        86304.000000   \n",
      "mean   ...         26.428230        15.023764           22.292364   \n",
      "std    ...         28.463560         7.569480           15.324974   \n",
      "min    ...          5.000114         4.960537            1.960516   \n",
      "25%    ...         12.999058        10.004520            8.995295   \n",
      "50%    ...         17.000914        13.005257           20.997763   \n",
      "75%    ...         24.002790        17.514050           32.001019   \n",
      "max    ...        305.002451       187.999487          278.996229   \n",
      "\n",
      "       Run Time CF+0.25*PPM  Run Time CF+0.50*PPM  Run Time CF+0.75*PPM  \\\n",
      "count          86304.000000          86304.000000          86304.000000   \n",
      "mean               6.578661              6.697173              6.870780   \n",
      "std                2.409871              2.572184              2.742322   \n",
      "min                2.953529              2.959728              2.000093   \n",
      "25%                5.000114              4.999876              5.000114   \n",
      "50%                6.000996              6.001472              6.002903   \n",
      "75%                7.521093              7.997751              7.999897   \n",
      "max               99.549770             76.547623             59.000015   \n",
      "\n",
      "       Run Time CF+1*PPM  Run Time CF/PPM  Run Time Inference CF  \\\n",
      "count       86304.000000     86304.000000           86304.000000   \n",
      "mean            7.445004         6.478721              14.558654   \n",
      "std             2.743841         4.996045               7.291878   \n",
      "min             2.970219         2.960682               7.998228   \n",
      "25%             5.996227         4.999876              11.998653   \n",
      "50%             6.999969         6.000042              14.001846   \n",
      "75%             8.994818         7.005692              16.993046   \n",
      "max            64.002275      1300.812721            1692.997932   \n",
      "\n",
      "       Run Time Inference PPM  \n",
      "count            86304.000000  \n",
      "mean                13.686529  \n",
      "std                 10.114844  \n",
      "min                  8.508921  \n",
      "25%                 11.000156  \n",
      "50%                 12.995958  \n",
      "75%                 15.993357  \n",
      "max               1949.002266  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# do a statistical analysis on the test dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model_ppm = PPMModel_ppm()\n",
    "#model_ppm.load_state_dict(torch.load(\"resized_ppm.pth\")['state_dict'])\n",
    "\n",
    "model_cf = PPMModel_cf()\n",
    "#model_cf.load_state_dict(torch.load(\"resized_cf.pth\")['state_dict'])\n",
    "\n",
    "#model_ppm_cf = PPMModel()\n",
    "model_ppm.load_state_dict(torch.load(\"new_model_aug_ppm.pth\"))\n",
    "model_cf.load_state_dict(torch.load(\"new_model_aug_cf.pth\"))\n",
    "\n",
    "#ici\n",
    "\n",
    "model_ppm.eval()\n",
    "model_cf.eval()\n",
    "#model_ppm_cf.eval()\n",
    "\n",
    "results = []\n",
    "optimal_found_ppm_man = 0\n",
    "optimal_found_ppm_cf = 0\n",
    "optimal_found_cf = 0\n",
    "optimal_found_cf_ppm25 = 0\n",
    "optimal_found_cf_ppm50 = 0\n",
    "optimal_found_cf_ppm75 = 0\n",
    "optimal_found_cf_ppm1 = 0\n",
    "optimal_found_cf_sur_ppm = 0\n",
    "no_path = 0\n",
    "\n",
    "instance_complexity = 1\n",
    "\n",
    "test = create_dataset(\"../32x MP_dataset_unclassed_augmented_cf/test\")\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    x_test.append((test[i][0], test[i][1]))\n",
    "    y_test.append(test[i][2])\n",
    "\n",
    "# input3 is a tensor of 0\n",
    "#input3 = torch.zeros(1, 1, 32, 32)\n",
    "\n",
    "# n_steps = size of the test dataset\n",
    "n_steps = len(x_test)\n",
    "for i in tqdm.tqdm(range(n_steps)):\n",
    "    with torch.no_grad():\n",
    "        x_batch, y_batch = get_non_random_batch(x_test, y_test, 1, i)[0]\n",
    "        type_of_map = x_batch[0].split('/')[-1].split('__')[0]\n",
    "        input1 = torch.tensor(np.array([np.array(Image.open(x_batch[0])) / 255]), dtype=torch.float32).unsqueeze(1)\n",
    "        goal = torch.tensor(np.array([np.array(Image.open(x_batch[1])) / 255]), dtype=torch.float32).unsqueeze(1)\n",
    "        goal_pos = np.where(goal[0][0] == 1)\n",
    "        goal_pos = (goal_pos[0][0], goal_pos[1][0])\n",
    "        \n",
    "        for _ in range(3):\n",
    "            start = goal_pos\n",
    "            while manhattan_distance(start, goal_pos) < 30 or input1[0][0][start[0], start[1]] == 0:\n",
    "                start = (np.random.randint(0, 32), np.random.randint(0, 32))\n",
    "            \n",
    "            # start_goal is goal image and we add the start position to it\n",
    "            image_goal = np.array(Image.open(x_batch[1])) / 255\n",
    "            # add start\n",
    "            image_goal[start[0], start[1]] = 1\n",
    "            start_goal = torch.tensor(np.array([image_goal]), dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            input_cf = torch.cat((input1, goal), dim=1)\n",
    "            \n",
    "            input_ppm = torch.cat((input1, start_goal), dim=1)\n",
    "            \n",
    "            rt_inf_ppm = time.time()\n",
    "            output_ppm = model_ppm(input_ppm)\n",
    "            rt_inf_ppm = time.time() - rt_inf_ppm\n",
    "            output_ppm = output_ppm.detach().squeeze().numpy()\n",
    "            \n",
    "            rt_inf_cf = time.time()\n",
    "            output_cf = model_cf(input_cf)\n",
    "            rt_inf_cf = time.time() - rt_inf_cf\n",
    "            output_cf = output_cf.detach().squeeze().numpy()\n",
    "            \n",
    "            path_cf, path_ppm_man, path_ppm_cf, path_GT, path_man, path_cf_ppm25, path_cf_ppm50, path_cf_ppm75, path_cf_ppm1, path_cf_sur_ppm, vis_cf, vis_ppm_man, vis_ppm_cf, vis_GT, vis_man, vis_cf_ppm25, vis_cf_ppm50, vis_cf_ppm75, vis_cf_ppm1, vis_cf_sur_ppm, rt_cf, rt_ppm_man, rt_ppm_cf, rt_man, rt_cf_ppm_25, rt_cf_ppm_50, rt_cf_ppm_75, rt_cf_ppm_1, rt_cf_sur_ppm = everything_new(2, input1[0][0], start, goal_pos, output_ppm, output_cf, 1.25)\n",
    "            \n",
    "            if path_cf is None or path_ppm_man is None or path_ppm_cf is None or path_GT is None or path_man is None or path_cf_ppm25 is None or path_cf_ppm50 is None or path_cf_ppm75 is None or path_cf_ppm1 is None or path_cf_sur_ppm is None:\n",
    "                no_path += 1\n",
    "                continue\n",
    "            \n",
    "            #print(path_pred_cf, path_pred_ppm, path_pred_ppm_cf, path_pred_ppm_x_cf, path_GT, path_manhattan, visited_pred_cf, visited_pred_ppm, visited_pred_ppm_cf, visited_pred_ppm_x_cf, visited_GT, visited_manhattan, rt_cf, rt_ppm, rt_ppm_cf, rt_ppm_x_cf, rt_manhattan)\n",
    "            \n",
    "            optimal_found_ppm_man = 0\n",
    "            optimal_found_ppm_cf = 0\n",
    "            optimal_found_cf = 0\n",
    "            optimal_found_cf_ppm25 = 0\n",
    "            optimal_found_cf_ppm50 = 0\n",
    "            optimal_found_cf_ppm75 = 0\n",
    "            optimal_found_cf_ppm1 = 0\n",
    "            optimal_found_cf_sur_ppm = 0\n",
    "            # instance complexity is defined by cost(π∗(start, goal))/h(start)\n",
    "            instance_complexity = path_GT / manhattan_distance(start, goal_pos)\n",
    "            \n",
    "            if path_GT == path_cf:\n",
    "                optimal_found_cf = 1\n",
    "            if path_GT == path_ppm_man:\n",
    "                optimal_found_ppm_man = 1\n",
    "            if path_GT == path_ppm_cf:\n",
    "                optimal_found_ppm_cf = 1\n",
    "            if path_GT == path_cf_ppm25:\n",
    "                optimal_found_cf_ppm25 = 1\n",
    "            if path_GT == path_cf_ppm50:\n",
    "                optimal_found_cf_ppm50 = 1\n",
    "            if path_GT == path_cf_ppm75:\n",
    "                optimal_found_cf_ppm75 = 1\n",
    "            if path_GT == path_cf_ppm1:\n",
    "                optimal_found_cf_ppm1 = 1\n",
    "            if path_GT == path_cf_sur_ppm:\n",
    "                optimal_found_cf_sur_ppm = 1\n",
    "                \n",
    "            results.append([type_of_map, instance_complexity, 100*optimal_found_cf, 100*optimal_found_ppm_man, 100*optimal_found_ppm_cf, 100*optimal_found_cf_ppm25, 100*optimal_found_cf_ppm50, 100*optimal_found_cf_ppm75, 100*optimal_found_cf_ppm1, 100*optimal_found_cf_sur_ppm, path_cf, path_ppm_man, path_ppm_cf, path_cf_ppm25, path_cf_ppm50, path_cf_ppm75, path_cf_ppm1, path_cf_sur_ppm, path_GT, path_man, vis_cf, vis_ppm_man, vis_ppm_cf, vis_cf_ppm25, vis_cf_ppm50, vis_cf_ppm75, vis_cf_ppm1, vis_cf_sur_ppm, vis_GT, vis_man, 1000*rt_cf, 1000*rt_ppm_man, 1000*rt_ppm_cf, 1000*rt_man, 1000*rt_cf_ppm_25, 1000*rt_cf_ppm_50, 1000*rt_cf_ppm_75, 1000*rt_cf_ppm_1, 1000*rt_cf_sur_ppm, 1000*rt_inf_cf, 1000*rt_inf_ppm])\n",
    "\n",
    "results = pd.DataFrame(results, columns=['Type of map', 'Instance Complexity', 'Optimal Found CF', 'Optimal Found PPM+MAN', 'Optimal Found PPM+CF', 'Optimal Found CF+0.25*PPM', 'Optimal Found CF+0.50*PPM', 'Optimal Found CF+0.75*PPM', 'Optimal Found CF+1*PPM', 'Optimal Found CF/PPM', 'Predicted Path Length CF', 'Predicted Path Length PPM+MAN', 'Predicted Path Length PPM+CF', 'Predicted Path Length CF+0.25*PPM', 'Predicted Path Length CF+0.50*PPM', 'Predicted Path Length CF+0.75*PPM', 'Predicted Path Length CF+1*PPM', 'Predicted Path Length CF/PPM', 'Ground Truth Path Length', 'Manhattan Path Length', 'Expansion CF', 'Expansion PPM+MAN', 'Expansion PPM+CF', 'Expansion CF+0.25*PPM', 'Expansion CF+0.50*PPM', 'Expansion CF+0.75*PPM', 'Expansion CF+1*PPM', 'Expansion CF/PPM', 'Expansion GT', 'Expansion Manhattan', 'Run Time CF', 'Run Time PPM+MAN', 'Run Time PPM+CF', 'Run Time Manhattan', 'Run Time CF+0.25*PPM', 'Run Time CF+0.50*PPM', 'Run Time CF+0.75*PPM', 'Run Time CF+1*PPM', 'Run Time CF/PPM', 'Run Time Inference CF', 'Run Time Inference PPM'])\n",
    "\n",
    "print(\"No path found:\", no_path)\n",
    "print(results.describe())\n",
    "\n",
    "# print describe all the things tha contains CF/PPM in the name of the column\n",
    "#print(results.filter(like='CF/PPM').describe())\n",
    "#print(results.filter(like='Exp').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results in a csv file\n",
    "results.to_csv(\"no_w_espoir_final_32.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiled-MP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, root, rem512=False, rem256=False, rem16=False, uniform=False, rem128=False):\n",
    "        files = []\n",
    "        dict = {}\n",
    "        \n",
    "        def get_base_filename(filename):\n",
    "            return filename.split('_start')[0] + '.png'\n",
    "        \n",
    "        for file in glob.glob(root + '/*'):\n",
    "            if rem512:\n",
    "                if \"512x512\" in file:\n",
    "                    continue\n",
    "            if rem256:\n",
    "                if \"256x256\" in file:\n",
    "                    continue\n",
    "            if rem16:\n",
    "                if \"16x16\" in file:\n",
    "                    continue\n",
    "            if rem128:\n",
    "                if \"128x128\" in file:\n",
    "                    continue\n",
    "            if \"GT\" not in file and \"start\" not in file:\n",
    "                base_filename = file.split(\".\")[0]\n",
    "                for i in range(2):\n",
    "                    files.append(base_filename + \"_\" + str(i) + \".png\")\n",
    "            elif \"start\" in file:\n",
    "                base_filename = get_base_filename(file)\n",
    "                dict[base_filename] = file\n",
    "        \n",
    "        if uniform:\n",
    "            files_16, files_32, files_64, files_96, files_128, files_256, files_512 = [], [], [], [], [], [], []\n",
    "            for file in files:\n",
    "                if \"16x16\" in file:\n",
    "                    files_16.append(file)\n",
    "                elif \"32x32\" in file:\n",
    "                    files_32.append(file)\n",
    "                elif \"64x64\" in file:\n",
    "                    files_64.append(file)\n",
    "                elif \"96x96\" in file:\n",
    "                    files_96.append(file)\n",
    "                elif \"128x128\" in file:\n",
    "                    files_128.append(file)\n",
    "                elif \"256x256\" in file:\n",
    "                    files_256.append(file)\n",
    "                elif \"512x512\" in file:\n",
    "                    files_512.append(file)\n",
    "                \n",
    "            min = 200 # TODO: change \n",
    "            for files in [files_16, files_32, files_64, files_96, files_128, files_256, files_512]:\n",
    "                if len(files) < min and len(files) > 0:\n",
    "                    min = len(files)\n",
    "            if len(files_16) > 0:\n",
    "                files_16 = np.random.choice(files_16, min, replace=False)\n",
    "            if len(files_32) > 0:\n",
    "                files_32 = np.random.choice(files_32, min, replace=False)\n",
    "            if len(files_64) > 0:\n",
    "                files_64 = np.random.choice(files_64, min, replace=False)\n",
    "            if len(files_96) > 0:\n",
    "                files_96 = np.random.choice(files_96, min, replace=False)\n",
    "            if len(files_128) > 0:\n",
    "                files_128 = np.random.choice(files_128, min, replace=False)\n",
    "            if len(files_256) > 0:\n",
    "                files_256 = np.random.choice(files_256, min, replace=False)\n",
    "            if len(files_512) > 0:\n",
    "                files_512 = np.random.choice(files_512, min, replace=False)\n",
    "            files = list(files_16) + list(files_32) + list(files_64) + list(files_96) + list(files_128) + list(files_256) + list(files_512)\n",
    "        \n",
    "        self.inputs1 = []\n",
    "        self.inputs2 = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for file in tqdm.tqdm(files):\n",
    "            name = file.split('.')[0]\n",
    "            self.inputs1.append(name[:-2] + \".png\")\n",
    "            self.inputs2.append(dict[file])\n",
    "            self.targets.append([name + \"_GT_cf.png\", name + \"_GT_ppm.png\"])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input1 = cv2.imread(self.inputs1[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        input2 = cv2.imread(self.inputs2[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        targets = [cv2.imread(target, cv2.IMREAD_GRAYSCALE) for target in self.targets[idx]]\n",
    "        \n",
    "        goal = self.inputs2[idx].split('_end-')[1].split('.png')[0].split('_')\n",
    "        goal = (int(goal[0]), int(goal[1]))\n",
    "        # create an image the same size as input1 with the goal point\n",
    "        input3 = np.zeros(input1.shape, dtype=np.float32)\n",
    "        input3[goal[0], goal[1]] = 1\n",
    "        input3 = input3.astype('float32')\n",
    "        \n",
    "        input1 = input1.astype('float32')\n",
    "        input2 = input2.astype('float32')\n",
    "        targets = [target.astype('float32') for target in targets]\n",
    "        \n",
    "        return input1, input2, input3, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 200953.62it/s]\n"
     ]
    }
   ],
   "source": [
    "resized_test_dataset = Dataset(\"C:/Users/Elie/Desktop/Individual Research Project/Data/MP dataset/32x MP_dataset_unclassed_resized_all/test\", rem512=True, rem256=False, rem16=True, uniform=True, rem128=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class Loader_non_resize:\n",
    "    def __init__(self, dataset, batch_size, device):\n",
    "        self.device = device\n",
    "        split_indices = list(range(len(dataset)))\n",
    "        sampler = torch.utils.data.sampler.SubsetRandomSampler(split_indices)\n",
    "        self.loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=0, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield [d.to(self.device) for d in data]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images1 = [Image.fromarray((item[0]).astype('uint8')) for item in batch]\n",
    "        images2 = [Image.fromarray((item[1]).astype('uint8')) for item in batch]\n",
    "        images3 = [Image.fromarray((item[2]).astype('uint8')) for item in batch]\n",
    "        targets = [Image.fromarray((target).astype('uint8')) for item in batch for target in item[3]]\n",
    "        \n",
    "        # to tensor\n",
    "        transform = transforms.ToTensor()\n",
    "        images1 = [transform(image) for image in images1]\n",
    "        images2 = [transform(image) for image in images2]\n",
    "        images3 = [transform(image) for image in images3]\n",
    "        targets = [transform(image) for image in targets]\n",
    "\n",
    "        # return torch stack\n",
    "        return torch.stack(images1), torch.stack(images2), torch.stack(images3), torch.stack(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Procedure on Tiled-MP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:45:17<00:00, 24.32s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found: 39\n",
      "       Instance Complexity  Optimal Found CF  Optimal Found PPM+MAN  \\\n",
      "count           961.000000        961.000000             961.000000   \n",
      "mean              1.103907         22.164412              46.409990   \n",
      "std               0.190588         41.556914              49.896919   \n",
      "min               1.002336          0.000000               0.000000   \n",
      "25%               1.010870          0.000000               0.000000   \n",
      "50%               1.027778          0.000000               0.000000   \n",
      "75%               1.103448          0.000000             100.000000   \n",
      "max               2.750000        100.000000             100.000000   \n",
      "\n",
      "       Optimal Found PPM+CF  Optimal Found CF+0.25*PPM  \\\n",
      "count            961.000000                 961.000000   \n",
      "mean              42.559834                  47.970864   \n",
      "std               49.469086                  49.984823   \n",
      "min                0.000000                   0.000000   \n",
      "25%                0.000000                   0.000000   \n",
      "50%                0.000000                   0.000000   \n",
      "75%              100.000000                 100.000000   \n",
      "max              100.000000                 100.000000   \n",
      "\n",
      "       Optimal Found CF+0.50*PPM  Optimal Found CF+0.75*PPM  \\\n",
      "count                 961.000000                 961.000000   \n",
      "mean                   49.219563                  50.364204   \n",
      "std                    50.019941                  50.024708   \n",
      "min                     0.000000                   0.000000   \n",
      "25%                     0.000000                   0.000000   \n",
      "50%                     0.000000                 100.000000   \n",
      "75%                   100.000000                 100.000000   \n",
      "max                   100.000000                 100.000000   \n",
      "\n",
      "       Optimal Found CF+1*PPM  Optimal Found CF/PPM  Predicted Path Length CF  \\\n",
      "count              961.000000            961.000000                961.000000   \n",
      "mean                50.676379             22.892820                131.845994   \n",
      "std                 50.021457             42.036169                 96.883592   \n",
      "min                  0.000000              0.000000                 19.000000   \n",
      "25%                  0.000000              0.000000                 62.000000   \n",
      "50%                100.000000              0.000000                104.000000   \n",
      "75%                100.000000              0.000000                169.000000   \n",
      "max                100.000000            100.000000                571.000000   \n",
      "\n",
      "       ...  Run Time PPM+MAN  Run Time PPM+CF  Run Time Manhattan  \\\n",
      "count  ...        961.000000       961.000000          961.000000   \n",
      "mean   ...       1975.683932       748.738631          821.997299   \n",
      "std    ...       4906.082335      1630.114296         1579.179822   \n",
      "min    ...         10.995626         8.001328            6.998539   \n",
      "25%    ...         61.028957        53.003073           84.520817   \n",
      "50%    ...        139.998674       113.996983          256.004333   \n",
      "75%    ...        631.995678       551.987648          786.195755   \n",
      "max    ...      43385.115147     16197.552204        14734.407425   \n",
      "\n",
      "       Run Time CF+0.25*PPM  Run Time CF+0.50*PPM  Run Time CF+0.75*PPM  \\\n",
      "count            961.000000            961.000000            961.000000   \n",
      "mean             185.383517            171.421744            170.974184   \n",
      "std              301.879140            291.968858            281.339341   \n",
      "min                5.996704              6.967545              6.996870   \n",
      "25%               33.004522             31.999826             33.017159   \n",
      "50%               64.003229             60.994387             62.999964   \n",
      "75%              218.997002            186.995268            185.069323   \n",
      "max             4653.000355           4616.999626           4176.000834   \n",
      "\n",
      "       Run Time CF+1*PPM  Run Time CF/PPM  Run Time Inference CF  \\\n",
      "count         961.000000       961.000000             961.000000   \n",
      "mean          172.405636       180.429808              18.140960   \n",
      "std           280.904419       264.362265              53.636035   \n",
      "min             6.999969         6.999731               9.996891   \n",
      "25%            33.999205        37.004232              12.006998   \n",
      "50%            65.101385        82.523108              14.530897   \n",
      "75%           198.769331       217.000961              18.005133   \n",
      "max          4357.999802      4066.000223            1663.999319   \n",
      "\n",
      "       Run Time Inference PPM  \n",
      "count              961.000000  \n",
      "mean                15.253800  \n",
      "std                  5.469611  \n",
      "min                  9.969234  \n",
      "25%                 11.997938  \n",
      "50%                 13.995171  \n",
      "75%                 16.999483  \n",
      "max                 73.539972  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# do a statistical analysis on the test dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "model_ppm = PPMModel_ppm()\n",
    "model_cf = PPMModel_cf()\n",
    "#model_ppm_cf = PPMModel()\n",
    "\n",
    "#model_ppm.load_state_dict(torch.load(\"new_model_aug_ppm.pth\"))\n",
    "model_ppm.load_state_dict(torch.load(\"resized_ppm.pth\")[\"state_dict\"])\n",
    "\n",
    "model_cf.load_state_dict(torch.load(\"resized_cf.pth\")[\"state_dict\"])\n",
    "\n",
    "#model_ppm_cf.load_state_dict(torch.load(\"new_model_aug_both.pth\"))\n",
    "\n",
    "model_ppm.to('cuda')\n",
    "model_cf.to('cuda')\n",
    "#model_ppm_cf.to('cuda')\n",
    "\n",
    "model_ppm.eval()\n",
    "model_cf.eval()\n",
    "#model_ppm_cf.eval()\n",
    "\n",
    "# create Loader\n",
    "loader = Loader_non_resize(resized_test_dataset, 1, 'cuda')\n",
    "\n",
    "\"\"\"\n",
    "# take 1 using next\n",
    "input1, input2, input3, targets = next(iter(loader))\n",
    "\n",
    "# plot everything\n",
    "fig, ax = plt.subplots(1, 6, figsize=(15, 5))\n",
    "ax[0].imshow(input1[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[0].set_title('Input 1')\n",
    "ax[1].imshow(input2[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[1].set_title('Input 2')\n",
    "ax[2].imshow(input3[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[2].set_title('Input 3')\n",
    "ax[3].imshow(targets[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[3].set_title('Target CF')\n",
    "ax[4].imshow(targets[1][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[4].set_title('Target PPM')\n",
    "ax[5].imshow(targets[2][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[5].set_title('Target PPM+CF')\n",
    "plt.show()\n",
    "\n",
    "input1, input2, input3, targets = input1.to('cuda'), input2.to('cuda'), input3.to('cuda'), targets.to('cuda')\n",
    "\n",
    "input_cf = torch.cat((input1, input3), dim=1)\n",
    "input_ppm = torch.cat((input1, input2), dim=1)\n",
    "input_ppm_cf = torch.cat((input1, input2, input3), dim=1)\n",
    "\n",
    "output_cf = model_cf(input_cf)\n",
    "output_ppm = model_ppm(input_ppm)\n",
    "output_ppm_cf = model_ppm_cf(input_ppm_cf)\n",
    "\n",
    "# plot everything\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(output_cf[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[0].set_title('Output CF')\n",
    "ax[1].imshow(output_ppm[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[1].set_title('Output PPM')\n",
    "ax[2].imshow(output_ppm_cf[0][0].cpu().detach().numpy(), cmap='gray')\n",
    "ax[2].set_title('Output PPM+CF')\n",
    "plt.show() \n",
    "\"\"\"\n",
    "\n",
    "# ici\n",
    "\n",
    "results = []\n",
    "optimal_found_cf = 0\n",
    "optimal_found_ppm_man = 0\n",
    "optimal_found_ppm_cf = 0\n",
    "optimal_found_cf_ppm25 = 0\n",
    "optimal_found_cf_ppm50 = 0\n",
    "optimal_found_cf_ppm75 = 0\n",
    "optimal_found_cf_ppm1 = 0\n",
    "optimal_found_cf_sur_ppm = 0\n",
    "no_path = 0\n",
    "\n",
    "instance_complexity = 1\n",
    "\n",
    "for input1, input2, input3, target in tqdm.tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        size = str(input1[0][0].size()[0])\n",
    "        # create input4: tensor of 0s with the same size as input1\n",
    "        input4 = torch.zeros_like(input1)\n",
    "        input_cf = torch.cat((input1, input3, input4), dim=1) #, input4), dim=1)\n",
    "        input_ppm = torch.cat((input1, input2, input4), dim=1) #, input4), dim=1)\n",
    "        #input_ppm_cf = torch.cat((input1, input2, input3), dim=1)\n",
    "        start_time_inference_cf = time.time()\n",
    "        output_cf = model_cf(input_cf)\n",
    "        rt_inference_cf = time.time() - start_time_inference_cf\n",
    "        start_time_inference_ppm = time.time()\n",
    "        output_ppm = model_ppm(input_ppm)\n",
    "        rt_inference_ppm = time.time() - start_time_inference_ppm\n",
    "        #start_time_inference_ppm_cf = time.time()\n",
    "        #output_ppm_cf = model_ppm_cf(input_ppm_cf)\n",
    "        #rt_inference_ppm_cf = time.time() - start_time_inference_ppm_cf\n",
    "        \n",
    "        input2 = input2.detach().cpu().numpy()\n",
    "        input3 = input3.detach().cpu().numpy() * 255\n",
    "        goal = np.where(input3[0][0] == 1)\n",
    "        goal_pos = (goal[0][0], goal[1][0])\n",
    "        # start is the point where input 3 is 1 that is not the goal\n",
    "        start = np.where(input2[0][0] != 0)\n",
    "        if start[0][0] == goal_pos[0] and start[1][0] == goal_pos[1]:\n",
    "            start = (start[0][1], start[1][1])\n",
    "        else:\n",
    "            start = (start[0][0], start[1][0])\n",
    "        \n",
    "        #rt_inference_ppm_x_cf = rt_inference_ppm + rt_inference_cf\n",
    "        #start_time_inference_ppm_x_cf = time.time()\n",
    "        #output_ppm_x_cf = output_ppm * output_cf\n",
    "        #rt_inference_ppm_x_cf = time.time() - start_time_inference_ppm_x_cf + rt_inference_ppm_x_cf\n",
    "        \n",
    "        #output_ppm_x_cf = output_ppm_x_cf.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        # detach cpu unsqueeze and numpy the outputs\n",
    "        output_ppm = output_ppm.detach().cpu().numpy().squeeze()\n",
    "        output_cf = output_cf.detach().cpu().numpy().squeeze()\n",
    "        #output_ppm_cf = output_ppm_cf.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        optimal_found_cf = 0\n",
    "        optimal_found_ppm_man = 0\n",
    "        optimal_found_ppm_cf = 0\n",
    "        optimal_found_cf_ppm25 = 0\n",
    "        optimal_found_cf_ppm50 = 0\n",
    "        optimal_found_cf_ppm75 = 0\n",
    "        optimal_found_cf_ppm1 = 0\n",
    "        optimal_found_cf_sur_ppm = 0\n",
    "        \n",
    "        # use everything_new function\n",
    "        path_cf, path_ppm_man, path_ppm_cf, path_GT, path_man, path_cf_ppm25, path_cf_ppm50, path_cf_ppm75, path_cf_ppm1, path_cf_sur_ppm, vis_cf, vis_ppm_man, vis_ppm_cf, vis_GT, vis_man, vis_cf_ppm25, vis_cf_ppm50, vis_cf_ppm75, vis_cf_ppm1, vis_cf_sur_ppm, rt_cf, rt_ppm_man, rt_ppm_cf, rt_man, rt_cf_ppm_25, rt_cf_ppm_50, rt_cf_ppm_75, rt_cf_ppm_1, rt_cf_sur_ppm = everything_new(2, input1[0][0], start, goal_pos, output_ppm, output_cf, 1.25)\n",
    "        \n",
    "        if path_GT == None:\n",
    "            no_path += 1\n",
    "            continue\n",
    "        \n",
    "        # instance complexity is defined by cost(π∗(start, goal))/h(start)\n",
    "        instance_complexity = path_GT / manhattan_distance(start, goal_pos)\n",
    "        \n",
    "        if path_cf == path_GT:\n",
    "            optimal_found_cf = 1\n",
    "        if path_ppm_man == path_GT:\n",
    "            optimal_found_ppm_man = 1\n",
    "        if path_ppm_cf == path_GT:\n",
    "            optimal_found_ppm_cf = 1\n",
    "        if path_cf_ppm25 == path_GT:\n",
    "            optimal_found_cf_ppm25 = 1\n",
    "        if path_cf_ppm50 == path_GT:\n",
    "            optimal_found_cf_ppm50 = 1\n",
    "        if path_cf_ppm75 == path_GT:\n",
    "            optimal_found_cf_ppm75 = 1\n",
    "        if path_cf_ppm1 == path_GT:\n",
    "            optimal_found_cf_ppm1 = 1\n",
    "        if path_cf_sur_ppm == path_GT:\n",
    "            optimal_found_cf_sur_ppm = 1\n",
    "        \n",
    "        results.append([size, instance_complexity, 100*optimal_found_cf, 100*optimal_found_ppm_man, 100*optimal_found_ppm_cf, 100*optimal_found_cf_ppm25, 100*optimal_found_cf_ppm50, 100*optimal_found_cf_ppm75, 100*optimal_found_cf_ppm1, 100*optimal_found_cf_sur_ppm, path_cf, path_ppm_man, path_ppm_cf, path_cf_ppm25, path_cf_ppm50, path_cf_ppm75, path_cf_ppm1, path_cf_sur_ppm, path_GT, path_man, vis_cf, vis_ppm_man, vis_ppm_cf, vis_cf_ppm25, vis_cf_ppm50, vis_cf_ppm75, vis_cf_ppm1, vis_cf_sur_ppm, vis_GT, vis_man, 1000*rt_cf, 1000*rt_ppm_man, 1000*rt_ppm_cf, 1000*rt_man, 1000*rt_cf_ppm_25, 1000*rt_cf_ppm_50, 1000*rt_cf_ppm_75, 1000*rt_cf_ppm_1, 1000*rt_cf_sur_ppm, 1000*rt_inference_cf, 1000*rt_inference_ppm])\n",
    "\n",
    "results = pd.DataFrame(results, columns=['Size', 'Instance Complexity', 'Optimal Found CF', 'Optimal Found PPM+MAN', 'Optimal Found PPM+CF', 'Optimal Found CF+0.25*PPM', 'Optimal Found CF+0.50*PPM', 'Optimal Found CF+0.75*PPM', 'Optimal Found CF+1*PPM', 'Optimal Found CF/PPM', 'Predicted Path Length CF', 'Predicted Path Length PPM+MAN', 'Predicted Path Length PPM+CF', 'Predicted Path Length CF+0.25*PPM', 'Predicted Path Length CF+0.50*PPM', 'Predicted Path Length CF+0.75*PPM', 'Predicted Path Length CF+1*PPM', 'Predicted Path Length CF/PPM', 'Ground Truth Path Length', 'Manhattan Path Length', 'Expansion CF', 'Expansion PPM+MAN', 'Expansion PPM+CF', 'Expansion CF+0.25*PPM', 'Expansion CF+0.50*PPM', 'Expansion CF+0.75*PPM', 'Expansion CF+1*PPM', 'Expansion CF/PPM', 'Expansion GT', 'Expansion Manhattan', 'Run Time CF', 'Run Time PPM+MAN', 'Run Time PPM+CF', 'Run Time Manhattan', 'Run Time CF+0.25*PPM', 'Run Time CF+0.50*PPM', 'Run Time CF+0.75*PPM', 'Run Time CF+1*PPM', 'Run Time CF/PPM', 'Run Time Inference CF', 'Run Time Inference PPM'])\n",
    "\n",
    "print(\"No path found:\", no_path)\n",
    "print(results.describe())\n",
    "\n",
    "# save \n",
    "results.to_csv(\"no_w_resized_espoir_results_on_new_dataset.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Procedure on out-of-distribution dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [2:50:56<00:00, 30.44s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found: 37\n",
      "       Instance Complexity  Optimal Found CF  Optimal Found PPM+MAN  \\\n",
      "count           300.000000        300.000000             300.000000   \n",
      "mean              1.207268         33.333333              31.333333   \n",
      "std               0.356416         47.219216              46.462365   \n",
      "min               1.002849          0.000000               0.000000   \n",
      "25%               1.022702          0.000000               0.000000   \n",
      "50%               1.071429          0.000000               0.000000   \n",
      "75%               1.250349        100.000000             100.000000   \n",
      "max               5.063636        100.000000             100.000000   \n",
      "\n",
      "       Optimal Found PPM+CF  Optimal Found CF+0.25*PPM  \\\n",
      "count            300.000000                 300.000000   \n",
      "mean              28.333333                  39.666667   \n",
      "std               45.136977                  49.002309   \n",
      "min                0.000000                   0.000000   \n",
      "25%                0.000000                   0.000000   \n",
      "50%                0.000000                   0.000000   \n",
      "75%              100.000000                 100.000000   \n",
      "max              100.000000                 100.000000   \n",
      "\n",
      "       Optimal Found CF+0.50*PPM  Optimal Found CF+0.75*PPM  \\\n",
      "count                 300.000000                 300.000000   \n",
      "mean                   39.666667                  40.000000   \n",
      "std                    49.002309                  49.071649   \n",
      "min                     0.000000                   0.000000   \n",
      "25%                     0.000000                   0.000000   \n",
      "50%                     0.000000                   0.000000   \n",
      "75%                   100.000000                 100.000000   \n",
      "max                   100.000000                 100.000000   \n",
      "\n",
      "       Optimal Found CF+1*PPM  Optimal Found CF/PPM  Predicted Path Length CF  \\\n",
      "count              300.000000            300.000000                300.000000   \n",
      "mean                39.666667             30.000000                172.833333   \n",
      "std                 49.002309             45.902325                127.419142   \n",
      "min                  0.000000              0.000000                 15.000000   \n",
      "25%                  0.000000              0.000000                 54.500000   \n",
      "50%                  0.000000              0.000000                152.500000   \n",
      "75%                100.000000            100.000000                266.250000   \n",
      "max                100.000000            100.000000                626.000000   \n",
      "\n",
      "       ...  Run Time PPM+MAN  Run Time PPM+CF  Run Time Manhattan  \\\n",
      "count  ...        300.000000       300.000000          300.000000   \n",
      "mean   ...       2937.363497      2479.651486          272.817870   \n",
      "std    ...      12152.426480     11135.073107          445.977425   \n",
      "min    ...          1.999855         2.999067            0.999689   \n",
      "25%    ...         24.000287        24.099350           18.999815   \n",
      "50%    ...        240.304232       204.524398           92.498064   \n",
      "75%    ...       1830.620468      1579.794288          299.115121   \n",
      "max    ...     163045.683622    175886.471987         2799.785376   \n",
      "\n",
      "       Run Time CF+0.25*PPM  Run Time CF+0.50*PPM  Run Time CF+0.75*PPM  \\\n",
      "count            300.000000            300.000000            300.000000   \n",
      "mean             105.351880            105.849887            114.943794   \n",
      "std              200.493287            203.439924            279.107736   \n",
      "min                1.999140              0.999928              1.000643   \n",
      "25%                8.024156              8.004725              8.730173   \n",
      "50%               39.736032             37.488222             40.274382   \n",
      "75%              116.268277            115.029454            118.129075   \n",
      "max             1746.981382           1745.294809           3639.488935   \n",
      "\n",
      "       Run Time CF+1*PPM  Run Time CF/PPM  Run Time Inference CF  \\\n",
      "count         300.000000       300.000000             300.000000   \n",
      "mean          108.783578        80.948337             934.774555   \n",
      "std           225.076538       146.351009             791.749828   \n",
      "min             0.997543         1.000643              21.002054   \n",
      "25%             8.999825         8.015871              90.894282   \n",
      "50%            41.022062        38.231611            1102.301002   \n",
      "75%           112.036586        91.515720            1489.878416   \n",
      "max          2154.556751      1306.340933            3104.041338   \n",
      "\n",
      "       Run Time Inference PPM  \n",
      "count              300.000000  \n",
      "mean               881.358631  \n",
      "std                777.982291  \n",
      "min                 16.521692  \n",
      "25%                 67.509234  \n",
      "50%               1002.746701  \n",
      "75%               1375.439644  \n",
      "max               3646.572828  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/Elie/Desktop/Individual Research Project/Data/out-of-distrib_dataset\"\n",
    "folders0 = [\"non_resized\"] #, \"resized256\", \"non_resized\"]\n",
    "folders1 = [\"bgmaps\", \"da2\", \"maze\", \"street\"]\n",
    "import os \n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "# import transforms\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "model_cf = PPMModel_cf()\n",
    "#model_cf.load_state_dict(torch.load(\"new_model_aug_cf.pth\"))\n",
    "model_cf.load_state_dict(torch.load(\"resized_cf.pth\")[\"state_dict\"])\n",
    "#new_model_aug_cf.pth\n",
    "\n",
    "model_ppm = PPMModel_ppm()\n",
    "#model_ppm.load_state_dict(torch.load(\"new_model_aug_ppm.pth\"))\n",
    "model_ppm.load_state_dict(torch.load(\"resized_ppm.pth\")[\"state_dict\"])\n",
    "\n",
    "#model_ppm_cf = PPMModel()\n",
    "#model_ppm_cf.load_state_dict(torch.load(\"new_model_aug_both.pth\"))\n",
    "\n",
    "model_cf.eval()\n",
    "model_ppm.eval()\n",
    "#model_ppm_cf.eval()\n",
    "\n",
    "model_cf, model_ppm = model_cf.to(\"cpu\"), model_ppm.to(\"cpu\")\n",
    "\n",
    "# take 1 random image in each folders1 or each folders0 of the path\n",
    "images = []\n",
    "fromim = {}\n",
    "for folder0 in folders0:\n",
    "    for folder1 in folders1:\n",
    "        temp_path = path + \"/\" + folder0 + \"/\" + folder1\n",
    "        files = os.listdir(temp_path)\n",
    "        for file in files:\n",
    "            images.append(temp_path + \"/\" + file)\n",
    "            fromim[temp_path + \"/\" + file] = folder1\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "no_path = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "instance_complexity = 1\n",
    "\n",
    "# for every image\n",
    "for image in tqdm.tqdm(images):\n",
    "    # load the image and get its size\n",
    "    img = Image.open(image).convert(\"L\")\n",
    "    name = image.split(\"/\")[-1]\n",
    "    fromimm = fromim[image]\n",
    "    size = img.size\n",
    "    \n",
    "    if size[0] > 300 or size[1] > 300:\n",
    "        # resize the image\n",
    "        img = img.resize((300, 300))\n",
    "        #as gray scale\n",
    "        img = img.convert(\"L\")\n",
    "        size = img.size\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                if img.getpixel((i, j)) < 128 and img.getpixel((i, j)) > 0:\n",
    "                    img.putpixel((i, j), 0) if np.random.rand() < 0.9 else 255\n",
    "                elif img.getpixel((i, j)) > 128 and img.getpixel((i, j)) < 255:\n",
    "                    img.putpixel((i, j), 255) if np.random.rand() < 0.9 else 0\n",
    "        img = img.convert(\"L\")\n",
    "    \n",
    "    # img to numpy\n",
    "    img_arr = np.array(img)\n",
    "    \n",
    "    # random goal point\n",
    "    goal = (random.randint(0, size[1] - 1), random.randint(0, size[0] - 1))\n",
    "    while img_arr[goal[0], goal[1]] == 0:\n",
    "        goal = (random.randint(0, size[1] - 1), random.randint(0, size[0] - 1))\n",
    "    \n",
    "    # same for start point\n",
    "    start = (random.randint(0, size[1] - 1), random.randint(0, size[0] - 1))\n",
    "    while img_arr[start[0], start[1]] == 0 or manhattan_distance(start, goal) < min(size[0], size[1]) / 4:\n",
    "        start = (random.randint(0, size[1] - 1), random.randint(0, size[0] - 1))\n",
    "    \n",
    "    size = str(size[0]) + \"x\" + str(size[1])\n",
    "    \n",
    "    img = transforms.ToTensor()(img).unsqueeze(0).float()\n",
    "    goal_image = torch.zeros_like(img)\n",
    "    goal_image[0, 0, goal[0], goal[1]] = 1\n",
    "    goal_image = goal_image.float()\n",
    "    \n",
    "    start_goal = torch.zeros_like(img)\n",
    "    start_goal[0, 0, start[0], start[1]] = 1\n",
    "    start_goal[0, 0, goal[0], goal[1]] = 1\n",
    "    start_goal = start_goal.float()\n",
    "    \n",
    "    # to device\n",
    "    img = img.to(device)\n",
    "    goal_image = goal_image.to(device)\n",
    "    start_goal = start_goal.to(device)\n",
    "    \n",
    "    # create tensor of 0s with the same size as img\n",
    "    new_input = torch.zeros_like(img)\n",
    "    \n",
    "    input_cf = torch.cat((img, goal_image, new_input), dim=1)\n",
    "    input_ppm = torch.cat((img, start_goal, new_input), dim=1)\n",
    "    #input_ppm_cf = torch.cat((img, start_goal, goal_image), dim=1)\n",
    "    \n",
    "    rt_inf_cf = time.time()\n",
    "    output_cf = model_cf(input_cf)\n",
    "    rt_inf_cf = time.time() - rt_inf_cf\n",
    "    rt_inf_ppm = time.time()\n",
    "    output_ppm = model_ppm(input_ppm)\n",
    "    rt_inf_ppm = time.time() - rt_inf_ppm\n",
    "    #output_ppm_cf = model_ppm_cf(input_ppm_cf)\n",
    "    \n",
    "    output_cf = output_cf.detach().cpu().numpy().squeeze()\n",
    "    output_ppm = output_ppm.detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    #ici\n",
    "    \n",
    "    optimal_found_ppm_man = 0\n",
    "    optimal_found_cf = 0\n",
    "    optimal_found_ppm_cf = 0\n",
    "    optimal_found_cf_ppm25 = 0\n",
    "    optimal_found_cf_ppm50 = 0\n",
    "    optimal_found_cf_ppm75 = 0\n",
    "    optimal_found_cf_ppm1 = 0\n",
    "    optimal_found_cf_sur_ppm = 0\n",
    "    \n",
    "    path_cf, path_ppm_man, path_ppm_cf, path_GT, path_man, path_cf_ppm25, path_cf_ppm50, path_cf_ppm75, path_cf_ppm1, path_cf_sur_ppm, vis_cf, vis_ppm_man, vis_ppm_cf, vis_GT, vis_man, vis_cf_ppm25, vis_cf_ppm50, vis_cf_ppm75, vis_cf_ppm1, vis_cf_sur_ppm, rt_cf, rt_ppm_man, rt_ppm_cf, rt_man, rt_cf_ppm_25, rt_cf_ppm_50, rt_cf_ppm_75, rt_cf_ppm_1, rt_cf_sur_ppm = everything_new(2, img[0][0], start, goal, output_ppm, output_cf, 1.25)\n",
    "    \n",
    "    if path_GT == None:\n",
    "        no_path += 1\n",
    "        continue\n",
    "    \n",
    "    instance_complexity = path_GT / manhattan_distance(start, goal)\n",
    "\n",
    "    if path_cf == path_GT:\n",
    "        optimal_found_cf = 1\n",
    "    if path_ppm_man == path_GT:\n",
    "        optimal_found_ppm_man = 1\n",
    "    if path_ppm_cf == path_GT:\n",
    "        optimal_found_ppm_cf = 1\n",
    "    if path_cf_ppm25 == path_GT:\n",
    "        optimal_found_cf_ppm25 = 1\n",
    "    if path_cf_ppm50 == path_GT:\n",
    "        optimal_found_cf_ppm50 = 1\n",
    "    if path_cf_ppm75 == path_GT:\n",
    "        optimal_found_cf_ppm75 = 1\n",
    "    if path_cf_ppm1 == path_GT:\n",
    "        optimal_found_cf_ppm1 = 1\n",
    "    if path_cf_sur_ppm == path_GT:\n",
    "        optimal_found_cf_sur_ppm = 1\n",
    "    \n",
    "    results.append([fromimm, name, size, instance_complexity, 100*optimal_found_cf, 100*optimal_found_ppm_man, 100*optimal_found_ppm_cf, 100*optimal_found_cf_ppm25, 100*optimal_found_cf_ppm50, 100*optimal_found_cf_ppm75, 100*optimal_found_cf_ppm1, 100*optimal_found_cf_sur_ppm, path_cf, path_ppm_man, path_ppm_cf, path_cf_ppm25, path_cf_ppm50, path_cf_ppm75, path_cf_ppm1, path_cf_sur_ppm, path_GT, path_man, vis_cf, vis_ppm_man, vis_ppm_cf, vis_cf_ppm25, vis_cf_ppm50, vis_cf_ppm75, vis_cf_ppm1, vis_cf_sur_ppm, vis_GT, vis_man, 1000*rt_cf, 1000*rt_ppm_man, 1000*rt_ppm_cf, 1000*rt_man, 1000*rt_cf_ppm_25, 1000*rt_cf_ppm_50, 1000*rt_cf_ppm_75, 1000*rt_cf_ppm_1, 1000*rt_cf_sur_ppm, 1000*rt_inf_cf, 1000*rt_inf_ppm])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(results, columns=['From', 'Name', 'Size', 'Instance Complexity', 'Optimal Found CF', 'Optimal Found PPM+MAN', 'Optimal Found PPM+CF', 'Optimal Found CF+0.25*PPM', 'Optimal Found CF+0.50*PPM', 'Optimal Found CF+0.75*PPM', 'Optimal Found CF+1*PPM', 'Optimal Found CF/PPM', 'Predicted Path Length CF', 'Predicted Path Length PPM+MAN', 'Predicted Path Length PPM+CF', 'Predicted Path Length CF+0.25*PPM', 'Predicted Path Length CF+0.50*PPM', 'Predicted Path Length CF+0.75*PPM', 'Predicted Path Length CF+1*PPM', 'Predicted Path Length CF/PPM', 'Ground Truth Path Length', 'Manhattan Path Length', 'Expansion CF', 'Expansion PPM+MAN', 'Expansion PPM+CF', 'Expansion CF+0.25*PPM', 'Expansion CF+0.50*PPM', 'Expansion CF+0.75*PPM', 'Expansion CF+1*PPM', 'Expansion CF/PPM', 'Expansion GT', 'Expansion Manhattan', 'Run Time CF', 'Run Time PPM+MAN', 'Run Time PPM+CF', 'Run Time Manhattan', 'Run Time CF+0.25*PPM', 'Run Time CF+0.50*PPM', 'Run Time CF+0.75*PPM', 'Run Time CF+1*PPM', 'Run Time CF/PPM', 'Run Time Inference CF', 'Run Time Inference PPM'])\n",
    "\n",
    "print(\"No path found:\", no_path)\n",
    "print(results.describe())\n",
    "\n",
    "# save \n",
    "results.to_csv(\"no_w_resized_espoir_out_of_distrib_NON_resized.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
